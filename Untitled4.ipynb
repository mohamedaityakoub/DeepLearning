{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMcYs5LQNJ4L0EG7NhUp6dk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohamedaityakoub/DeepLearning/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3qf3-RJ4qB8",
        "outputId": "314f2d46-20e6-48ce-b422-dd90351f8619"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/amhcd-data-64/tifinagh-images/\n",
            "/content\n",
            "Loaded 28182 samples with 33 unique classes.\n",
            "Train: 16908 samples, Validation: 5637 samples, Test: 5637 samples\n",
            "Epoch 0, Train Loss: 3.5033, Val Loss: 3.4965, Train Acc: 0.0402, Val Acc: 0.0373\n",
            "Epoch 10, Train Loss: 3.5027, Val Loss: 3.4959, Train Acc: 0.0517, Val Acc: 0.0507\n",
            "Epoch 20, Train Loss: 3.0723, Val Loss: 3.0399, Train Acc: 0.1092, Val Acc: 0.1071\n",
            "Epoch 30, Train Loss: 2.3568, Val Loss: 2.2820, Train Acc: 0.3073, Val Acc: 0.3005\n",
            "Epoch 40, Train Loss: 1.5920, Val Loss: 1.5806, Train Acc: 0.5126, Val Acc: 0.4965\n",
            "Epoch 50, Train Loss: 1.1199, Val Loss: 1.1154, Train Acc: 0.6568, Val Acc: 0.6459\n",
            "Epoch 60, Train Loss: 0.7913, Val Loss: 0.8261, Train Acc: 0.7638, Val Acc: 0.7351\n",
            "Epoch 70, Train Loss: 0.5762, Val Loss: 0.6839, Train Acc: 0.8227, Val Acc: 0.7884\n",
            "Epoch 80, Train Loss: 0.4326, Val Loss: 0.5548, Train Acc: 0.8696, Val Acc: 0.8198\n",
            "Epoch 90, Train Loss: 0.3332, Val Loss: 0.4721, Train Acc: 0.9084, Val Acc: 0.8474\n",
            "\n",
            "Rapport de classification (Test set) :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ya       0.84      0.95      0.89       171\n",
            "         yab       0.81      0.74      0.78       171\n",
            "        yach       0.89      0.83      0.86       171\n",
            "         yad       0.82      0.92      0.87       171\n",
            "        yadd       0.86      0.76      0.81       171\n",
            "         yae       0.97      0.88      0.92       171\n",
            "         yaf       0.78      0.94      0.85       171\n",
            "         yag       0.75      0.90      0.82       171\n",
            "        yagg       0.95      0.95      0.95       170\n",
            "        yagh       0.89      0.96      0.92       170\n",
            "         yah       0.94      0.81      0.87       171\n",
            "        yahh       0.83      0.95      0.88       171\n",
            "         yaj       0.94      0.64      0.76       171\n",
            "         yak       0.80      0.80      0.80       171\n",
            "        yakk       0.95      0.91      0.93       171\n",
            "         yal       0.84      0.82      0.83       170\n",
            "         yam       0.95      0.87      0.91       171\n",
            "         yan       0.78      0.92      0.84       170\n",
            "         yaq       0.89      0.81      0.85       171\n",
            "         yar       0.80      0.89      0.84       171\n",
            "        yarr       0.81      0.81      0.81       171\n",
            "         yas       0.70      0.57      0.63       171\n",
            "        yass       0.84      0.85      0.84       171\n",
            "         yat       0.89      0.90      0.90       171\n",
            "        yatt       0.77      0.73      0.75       171\n",
            "         yaw       0.85      0.91      0.88       171\n",
            "         yax       0.92      0.89      0.90       170\n",
            "         yay       0.82      0.86      0.84       171\n",
            "         yaz       0.87      0.73      0.79       171\n",
            "        yazz       0.77      0.82      0.79       171\n",
            "         yey       0.81      0.75      0.78       171\n",
            "          yi       0.67      0.85      0.75       170\n",
            "          yu       0.85      0.77      0.80       171\n",
            "\n",
            "    accuracy                           0.84      5637\n",
            "   macro avg       0.84      0.84      0.84      5637\n",
            "weighted avg       0.84      0.84      0.84      5637\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "class MultiClassNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        np.random.seed(42)\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            self.activations.append(relu(z))\n",
        "\n",
        "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "        self.z_values.append(z)\n",
        "        output = softmax(z)\n",
        "        self.activations.append(output)\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
        "        return loss\n",
        "\n",
        "    def compute_accuracy(self, y_true, y_pred):\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        true_labels = np.argmax(y_true, axis=1)\n",
        "        accuracy = np.mean(predictions == true_labels)\n",
        "        return accuracy\n",
        "\n",
        "    def backward(self, X, y, outputs):\n",
        "        m = X.shape[0]\n",
        "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        dZ = outputs - y\n",
        "        self.d_weights[-1] = (self.activations[-2].T @ dZ) / m\n",
        "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        for i in range(len(self.weights) - 2, -1, -1):\n",
        "            dA_prev = dZ @ self.weights[i+1].T\n",
        "            dZ = dA_prev * relu_derivative(self.z_values[i])\n",
        "            self.d_weights[i] = (self.activations[i].T @ dZ) / m\n",
        "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * self.d_weights[i]\n",
        "            self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
        "\n",
        "    def train(self, X, y, X_val, y_val, epochs, batch_size):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                outputs = self.forward(X_batch)\n",
        "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
        "                self.backward(X_batch, y_batch, outputs)\n",
        "\n",
        "            train_loss = epoch_loss / (X.shape[0] // batch_size)\n",
        "            train_pred = self.forward(X)\n",
        "            train_accuracy = self.compute_accuracy(y, train_pred)\n",
        "            val_pred = self.forward(X_val)\n",
        "            val_loss = self.compute_loss(y_val, val_pred)\n",
        "            val_accuracy = self.compute_accuracy(y_val, val_pred)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                      f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = self.forward(X)\n",
        "        predictions = np.argmax(outputs, axis=1)\n",
        "        return predictions\n",
        "\n",
        "data_dir = os.path.join(os.getcwd(), 'amhcd-data-64/tifinagh-images/')\n",
        "print(data_dir)\n",
        "current_working_directory = os.getcwd()\n",
        "print(current_working_directory)\n",
        "\n",
        "try:\n",
        "    labels_df = pd.read_csv(os.path.join(data_dir, 'amhcd-data-64/labels-map.csv'))\n",
        "except FileNotFoundError:\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for label_dir in os.listdir(data_dir):\n",
        "        label_path = os.path.join(data_dir, label_dir)\n",
        "        if os.path.isdir(label_path):\n",
        "            for img_name in os.listdir(label_path):\n",
        "                image_paths.append(os.path.join(label_path, img_name))\n",
        "                labels.append(label_dir)\n",
        "    labels_df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "\n",
        "print(f\"Loaded {len(labels_df)} samples with {labels_df['label'].nunique()} unique classes.\")\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['label'])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "def load_and_preprocess_image(image_path, target_size=(32, 32)):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, target_size)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    return img.flatten()\n",
        "\n",
        "X = np.array([load_and_preprocess_image(path) for path in labels_df['image_path']])\n",
        "y = labels_df['label_encoded'].values\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_val = np.array(X_val)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(f\"Train: {X_train.shape[0]} samples, Validation: {X_val.shape[0]} samples, Test: {X_test.shape[0]} samples\")\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_one_hot = np.array(one_hot_encoder.fit_transform(y_train.reshape(-1, 1)))\n",
        "y_val_one_hot = np.array(one_hot_encoder.transform(y_val.reshape(-1, 1)))\n",
        "y_test_one_hot = np.array(one_hot_encoder.transform(y_test.reshape(-1, 1)))\n",
        "\n",
        "layer_sizes = [X_train.shape[1], 64, 32, num_classes]\n",
        "nn = MultiClassNeuralNetwork(layer_sizes, learning_rate=0.01)\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
        "    X_train, y_train_one_hot, X_val, y_val_one_hot, epochs=100, batch_size=32\n",
        ")\n",
        "\n",
        "y_pred = nn.predict(X_test)\n",
        "print(\"\\nRapport de classification (Test set) :\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matrice de confusion (Test set)')\n",
        "plt.xlabel('Predit')\n",
        "plt.ylabel('Rel')\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ax1.plot(train_losses, label='Train Loss')\n",
        "ax1.plot(val_losses, label='Validation Loss')\n",
        "ax1.set_title('Courbe de perte')\n",
        "ax1.set_xlabel('Epoque')\n",
        "ax1.set_ylabel('Perte')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(train_accuracies, label='Train Accuracy')\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
        "ax2.set_title('Courbe de précision')\n",
        "ax2.set_xlabel('Epoque')\n",
        "ax2.set_ylabel('Précision')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('loss_accuracy_plot.png')\n",
        "plt.close()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZTYMQBMr6EXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Chemin du fichier ZIP\n",
        "zip_path = 'amhcd-data-64.zip'\n",
        "# Dossier de destination\n",
        "extract_folder = '/content'\n",
        "os.makedirs(extract_folder, exist_ok=True)\n",
        "\n",
        "# Extraction\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_folder)\n",
        "\n",
        "print(f\"Dossier extrait dans : {extract_folder}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0G52qbLm0mlF",
        "outputId": "3aeb4ac4-0d32-4791-8dcb-fb9058e0728f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dossier extrait dans : /content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "class MultiClassNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        np.random.seed(42)\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            self.activations.append(relu(z))\n",
        "\n",
        "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "        self.z_values.append(z)\n",
        "        output = softmax(z)\n",
        "        self.activations.append(output)\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
        "        return loss\n",
        "\n",
        "    def compute_accuracy(self, y_true, y_pred):\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        true_labels = np.argmax(y_true, axis=1)\n",
        "        accuracy = np.mean(predictions == true_labels)\n",
        "        return accuracy\n",
        "\n",
        "    def backward(self, X, y, outputs):\n",
        "        m = X.shape[0]\n",
        "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        dZ = outputs - y\n",
        "        self.d_weights[-1] = (self.activations[-2].T @ dZ) / m\n",
        "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        for i in range(len(self.weights) - 2, -1, -1):\n",
        "            dA_prev = dZ @ self.weights[i+1].T\n",
        "            dZ = dA_prev * relu_derivative(self.z_values[i])\n",
        "            self.d_weights[i] = (self.activations[i].T @ dZ) / m\n",
        "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * self.d_weights[i]\n",
        "            self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
        "\n",
        "    def train(self, X, y, X_val, y_val, epochs, batch_size):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                outputs = self.forward(X_batch)\n",
        "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
        "                self.backward(X_batch, y_batch, outputs)\n",
        "\n",
        "            train_loss = epoch_loss / (X.shape[0] // batch_size)\n",
        "            train_pred = self.forward(X)\n",
        "            train_accuracy = self.compute_accuracy(y, train_pred)\n",
        "            val_pred = self.forward(X_val)\n",
        "            val_loss = self.compute_loss(y_val, val_pred)\n",
        "            val_accuracy = self.compute_accuracy(y_val, val_pred)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                      f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = self.forward(X)\n",
        "        predictions = np.argmax(outputs, axis=1)\n",
        "        return predictions\n",
        "\n",
        "data_dir = os.path.join(os.getcwd(), 'amhcd-data-64/tifinagh-images/')\n",
        "print(data_dir)\n",
        "current_working_directory = os.getcwd()\n",
        "print(current_working_directory)\n",
        "\n",
        "try:\n",
        "    labels_df = pd.read_csv(os.path.join(data_dir, 'amhcd-data-64/labels-map.csv'))\n",
        "except FileNotFoundError:\n",
        "    image_paths = []\n",
        "    labels = []\n",
        "    for label_dir in os.listdir(data_dir):\n",
        "        label_path = os.path.join(data_dir, label_dir)\n",
        "        if os.path.isdir(label_path):\n",
        "            for img_name in os.listdir(label_path):\n",
        "                image_paths.append(os.path.join(label_path, img_name))\n",
        "                labels.append(label_dir)\n",
        "    labels_df = pd.DataFrame({'image_path': image_paths, 'label': labels})\n",
        "\n",
        "print(f\"Loaded {len(labels_df)} samples with {labels_df['label'].nunique()} unique classes.\")\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels_df['label_encoded'] = label_encoder.fit_transform(labels_df['label'])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "def load_and_preprocess_image(image_path, target_size=(32, 32)):\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = cv2.resize(img, target_size)\n",
        "    img = img.astype(np.float32) / 255.0\n",
        "    return img.flatten()\n",
        "\n",
        "X = np.array([load_and_preprocess_image(path) for path in labels_df['image_path']])\n",
        "y = labels_df['label_encoded'].values\n",
        "\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify=y_temp, random_state=42)\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_val = np.array(X_val)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_val = np.array(y_val)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "print(f\"Train: {X_train.shape[0]} samples, Validation: {X_val.shape[0]} samples, Test: {X_test.shape[0]} samples\")\n",
        "\n",
        "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_one_hot = np.array(one_hot_encoder.fit_transform(y_train.reshape(-1, 1)))\n",
        "y_val_one_hot = np.array(one_hot_encoder.transform(y_val.reshape(-1, 1)))\n",
        "y_test_one_hot = np.array(one_hot_encoder.transform(y_test.reshape(-1, 1)))\n",
        "\n",
        "layer_sizes = [X_train.shape[1], 64, 32, num_classes]\n",
        "nn = MultiClassNeuralNetwork(layer_sizes, learning_rate=0.1)\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
        "    X_train, y_train_one_hot, X_val, y_val_one_hot, epochs=100, batch_size=32\n",
        ")\n",
        "\n",
        "y_pred = nn.predict(X_test)\n",
        "print(\"\\nRapport de classification (Test set) :\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matrice de confusion (Test set)')\n",
        "plt.xlabel('Predit')\n",
        "plt.ylabel('Rel')\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ax1.plot(train_losses, label='Train Loss')\n",
        "ax1.plot(val_losses, label='Validation Loss')\n",
        "ax1.set_title('Courbe de perte')\n",
        "ax1.set_xlabel('Epoque')\n",
        "ax1.set_ylabel('Perte')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(train_accuracies, label='Train Accuracy')\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
        "ax2.set_title('Courbe de précision')\n",
        "ax2.set_xlabel('Epoque')\n",
        "ax2.set_ylabel('Précision')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('loss_accuracy_plot.png')\n",
        "plt.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwCxCRlP3h-Z",
        "outputId": "94195e65-5064-426e-a34d-a5b7581014f5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/amhcd-data-64/tifinagh-images/\n",
            "/content\n",
            "Loaded 28182 samples with 33 unique classes.\n",
            "Train: 16908 samples, Validation: 5637 samples, Test: 5637 samples\n",
            "Epoch 0, Train Loss: 3.5041, Val Loss: 3.4962, Train Acc: 0.0303, Val Acc: 0.0303\n",
            "Epoch 10, Train Loss: 0.4001, Val Loss: 0.4726, Train Acc: 0.8870, Val Acc: 0.8398\n",
            "Epoch 20, Train Loss: 0.1292, Val Loss: 0.4893, Train Acc: 0.9312, Val Acc: 0.8689\n",
            "Epoch 30, Train Loss: 0.0526, Val Loss: 0.3523, Train Acc: 0.9787, Val Acc: 0.9024\n",
            "Epoch 40, Train Loss: 0.0109, Val Loss: 0.2926, Train Acc: 0.9986, Val Acc: 0.9274\n",
            "Epoch 50, Train Loss: 0.0056, Val Loss: 0.3043, Train Acc: 0.9997, Val Acc: 0.9324\n",
            "Epoch 60, Train Loss: 0.0021, Val Loss: 0.3075, Train Acc: 0.9998, Val Acc: 0.9303\n",
            "Epoch 70, Train Loss: 0.1606, Val Loss: 1.2451, Train Acc: 0.8733, Val Acc: 0.8043\n",
            "Epoch 80, Train Loss: 0.0031, Val Loss: 0.3054, Train Acc: 0.9998, Val Acc: 0.9322\n",
            "Epoch 90, Train Loss: 0.0017, Val Loss: 0.3105, Train Acc: 0.9999, Val Acc: 0.9338\n",
            "\n",
            "Rapport de classification (Test set) :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ya       0.95      0.98      0.97       171\n",
            "         yab       0.91      0.92      0.92       171\n",
            "        yach       0.94      0.93      0.94       171\n",
            "         yad       0.97      0.96      0.96       171\n",
            "        yadd       0.89      0.89      0.89       171\n",
            "         yae       0.98      0.96      0.97       171\n",
            "         yaf       0.95      0.95      0.95       171\n",
            "         yag       0.97      0.94      0.96       171\n",
            "        yagg       0.97      0.99      0.98       170\n",
            "        yagh       0.92      0.98      0.95       170\n",
            "         yah       0.95      0.98      0.96       171\n",
            "        yahh       0.95      0.95      0.95       171\n",
            "         yaj       0.91      0.92      0.91       171\n",
            "         yak       0.95      0.93      0.94       171\n",
            "        yakk       0.98      0.94      0.96       171\n",
            "         yal       0.93      0.94      0.94       170\n",
            "         yam       0.94      0.94      0.94       171\n",
            "         yan       0.89      0.95      0.92       170\n",
            "         yaq       0.92      0.96      0.94       171\n",
            "         yar       0.90      0.91      0.90       171\n",
            "        yarr       0.93      0.90      0.91       171\n",
            "         yas       0.90      0.88      0.89       171\n",
            "        yass       0.92      0.93      0.93       171\n",
            "         yat       0.95      0.95      0.95       171\n",
            "        yatt       0.90      0.86      0.88       171\n",
            "         yaw       0.95      0.94      0.94       171\n",
            "         yax       0.96      0.96      0.96       170\n",
            "         yay       0.94      0.94      0.94       171\n",
            "         yaz       0.92      0.94      0.93       171\n",
            "        yazz       0.93      0.88      0.90       171\n",
            "         yey       0.88      0.89      0.89       171\n",
            "          yi       0.90      0.86      0.88       170\n",
            "          yu       0.86      0.87      0.86       171\n",
            "\n",
            "    accuracy                           0.93      5637\n",
            "   macro avg       0.93      0.93      0.93      5637\n",
            "weighted avg       0.93      0.93      0.93      5637\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [

        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "class MultiClassNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01, lambda_reg=0.0):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.lambda_reg = lambda_reg  # Added lambda parameter for regularization\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "\n",
        "        np.random.seed(42)\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            self.activations.append(relu(z))\n",
        "\n",
        "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "        self.z_values.append(z)\n",
        "        output = softmax(z)\n",
        "        self.activations.append(output)\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        cross_entropy_loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
        "\n",
        "        # Add L2 regularization term\n",
        "        l2_reg_loss = 0\n",
        "        for w in self.weights:\n",
        "            l2_reg_loss += np.sum(np.square(w))\n",
        "        l2_reg_loss = (self.lambda_reg / (2 * y_true.shape[0])) * l2_reg_loss\n",
        "\n",
        "        total_loss = cross_entropy_loss + l2_reg_loss\n",
        "        return total_loss\n",
        "\n",
        "    def compute_accuracy(self, y_true, y_pred):\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        true_labels = np.argmax(y_true, axis=1)\n",
        "        accuracy = np.mean(predictions == true_labels)\n",
        "        return accuracy\n",
        "\n",
        "    def backward(self, X, y, outputs):\n",
        "        m = X.shape[0]\n",
        "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        dZ = outputs - y\n",
        "        # Add L2 regularization gradient\n",
        "        self.d_weights[-1] = (self.activations[-2].T @ dZ) / m + (self.lambda_reg / m) * self.weights[-1]\n",
        "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        for i in range(len(self.weights) - 2, -1, -1):\n",
        "            dA_prev = dZ @ self.weights[i+1].T\n",
        "            dZ = dA_prev * relu_derivative(self.z_values[i])\n",
        "            # Add L2 regularization gradient\n",
        "            self.d_weights[i] = (self.activations[i].T @ dZ) / m + (self.lambda_reg / m) * self.weights[i]\n",
        "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        for i in range(len(self.weights)):\n",
        "            self.weights[i] -= self.learning_rate * self.d_weights[i]\n",
        "            self.biases[i] -= self.learning_rate * self.d_biases[i]\n",
        "\n",
        "    def train(self, X, y, X_val, y_val, epochs, batch_size):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                outputs = self.forward(X_batch)\n",
        "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
        "                self.backward(X_batch, y_batch, outputs)\n",
        "\n",
        "            train_loss = epoch_loss / (X.shape[0] // batch_size)\n",
        "            train_pred = self.forward(X)\n",
        "            train_accuracy = self.compute_accuracy(y, train_pred)\n",
        "            val_pred = self.forward(X_val)\n",
        "            val_loss = self.compute_loss(y_val, val_pred)\n",
        "            val_accuracy = self.compute_accuracy(y_val, val_pred)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                      f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = self.forward(X)\n",
        "        predictions = np.argmax(outputs, axis=1)\n",
        "        return predictions\n",
        "\n",
        "# Re-instantiate the model with lambda_reg (e.g., lambda_reg=0.001)\n",
        "layer_sizes = [X_train.shape[1], 64, 32, num_classes]\n",
        "nn = MultiClassNeuralNetwork(layer_sizes, learning_rate=0.1, lambda_reg=0.001) # Added lambda_reg\n",
        "\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
        "    X_train, y_train_one_hot, X_val, y_val_one_hot, epochs=100, batch_size=32\n",
        ")\n",
        "\n",
        "y_pred = nn.predict(X_test)\n",
        "print(\"\\nRapport de classification (Test set) :\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matrice de confusion (Test set)')\n",
        "plt.xlabel('Predit')\n",
        "plt.ylabel('Rel')\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ax1.plot(train_losses, label='Train Loss')\n",
        "ax1.plot(val_losses, label='Validation Loss')\n",
        "ax1.set_title('Courbe de perte')\n",
        "ax1.set_xlabel('Epoque')\n",
        "ax1.set_ylabel('Perte')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(train_accuracies, label='Train Accuracy')\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
        "ax2.set_title('Courbe de précision')\n",
        "ax2.set_xlabel('Epoque')\n",
        "ax2.set_ylabel('Précision')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('loss_accuracy_plot.png')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oyrSA-6UH3tc",
        "outputId": "9bafa06f-810c-4c7a-a3e4-cec25fca86e8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 3.5042, Val Loss: 3.4962, Train Acc: 0.0303, Val Acc: 0.0303\n",
            "Epoch 10, Train Loss: 0.4116, Val Loss: 0.4559, Train Acc: 0.8901, Val Acc: 0.8426\n",
            "Epoch 20, Train Loss: 0.1433, Val Loss: 0.3404, Train Acc: 0.9589, Val Acc: 0.8985\n",
            "Epoch 30, Train Loss: 0.0773, Val Loss: 0.2705, Train Acc: 0.9920, Val Acc: 0.9232\n",
            "Epoch 40, Train Loss: 0.0375, Val Loss: 0.2850, Train Acc: 0.9959, Val Acc: 0.9255\n",
            "Epoch 50, Train Loss: 0.0263, Val Loss: 0.2834, Train Acc: 0.9996, Val Acc: 0.9329\n",
            "Epoch 60, Train Loss: 0.0237, Val Loss: 0.2883, Train Acc: 0.9995, Val Acc: 0.9315\n",
            "Epoch 70, Train Loss: 0.0229, Val Loss: 0.2824, Train Acc: 0.9997, Val Acc: 0.9317\n",
            "Epoch 80, Train Loss: 0.0318, Val Loss: 0.2790, Train Acc: 0.9995, Val Acc: 0.9351\n",
            "Epoch 90, Train Loss: 0.0245, Val Loss: 0.2794, Train Acc: 0.9999, Val Acc: 0.9367\n",
            "\n",
            "Rapport de classification (Test set) :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ya       0.97      0.98      0.97       171\n",
            "         yab       0.92      0.93      0.92       171\n",
            "        yach       0.96      0.94      0.95       171\n",
            "         yad       0.95      0.92      0.93       171\n",
            "        yadd       0.91      0.85      0.88       171\n",
            "         yae       0.98      0.97      0.97       171\n",
            "         yaf       0.96      0.96      0.96       171\n",
            "         yag       0.99      0.92      0.95       171\n",
            "        yagg       0.97      0.99      0.98       170\n",
            "        yagh       0.94      0.99      0.97       170\n",
            "         yah       0.93      0.95      0.94       171\n",
            "        yahh       0.94      0.95      0.94       171\n",
            "         yaj       0.92      0.90      0.91       171\n",
            "         yak       0.93      0.95      0.94       171\n",
            "        yakk       0.99      0.94      0.96       171\n",
            "         yal       0.94      0.93      0.93       170\n",
            "         yam       0.94      0.95      0.94       171\n",
            "         yan       0.91      0.95      0.93       170\n",
            "         yaq       0.96      0.96      0.96       171\n",
            "         yar       0.92      0.92      0.92       171\n",
            "        yarr       0.92      0.94      0.93       171\n",
            "         yas       0.92      0.92      0.92       171\n",
            "        yass       0.95      0.94      0.95       171\n",
            "         yat       0.97      0.96      0.97       171\n",
            "        yatt       0.88      0.91      0.89       171\n",
            "         yaw       0.96      0.94      0.95       171\n",
            "         yax       0.95      0.98      0.97       170\n",
            "         yay       0.94      0.94      0.94       171\n",
            "         yaz       0.91      0.91      0.91       171\n",
            "        yazz       0.92      0.91      0.91       171\n",
            "         yey       0.86      0.89      0.88       171\n",
            "          yi       0.90      0.91      0.90       170\n",
            "          yu       0.89      0.89      0.89       171\n",
            "\n",
            "    accuracy                           0.94      5637\n",
            "   macro avg       0.94      0.94      0.94      5637\n",
            "weighted avg       0.94      0.94      0.94      5637\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [

        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "class MultiClassNeuralNetwork:\n",
        "    def __init__(self, layer_sizes, learning_rate=0.01, beta1=0.09, beta2=0.09, epsilon=1e-8, lambda_reg=0.0):\n",
        "        self.layer_sizes = layer_sizes\n",
        "        self.learning_rate = learning_rate\n",
        "        self.beta1 = beta1      # Adam parameter beta1\n",
        "        self.beta2 = beta2      # Adam parameter beta2\n",
        "        self.epsilon = epsilon  # Adam parameter epsilon\n",
        "        self.lambda_reg = lambda_reg  # Added lambda parameter for regularization\n",
        "        self.weights = []\n",
        "        self.biases = []\n",
        "        self.m_weights = [] # Adam first moment vector for weights\n",
        "        self.v_weights = [] # Adam second moment vector for weights\n",
        "        self.m_biases = []  # Adam first moment vector for biases\n",
        "        self.v_biases = []  # Adam second moment vector for biases\n",
        "        self.t = 0          # Adam timestep\n",
        "\n",
        "        np.random.seed(42)\n",
        "        for i in range(len(layer_sizes) - 1):\n",
        "            w = np.random.randn(layer_sizes[i], layer_sizes[i+1]) * 0.01\n",
        "            b = np.zeros((1, layer_sizes[i+1]))\n",
        "            self.weights.append(w)\n",
        "            self.biases.append(b)\n",
        "            self.m_weights.append(np.zeros_like(w))\n",
        "            self.v_weights.append(np.zeros_like(w))\n",
        "            self.m_biases.append(np.zeros_like(b))\n",
        "            self.v_biases.append(np.zeros_like(b))\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.activations = [X]\n",
        "        self.z_values = []\n",
        "\n",
        "        for i in range(len(self.weights) - 1):\n",
        "            z = np.dot(self.activations[-1], self.weights[i]) + self.biases[i]\n",
        "            self.z_values.append(z)\n",
        "            self.activations.append(relu(z))\n",
        "\n",
        "        z = np.dot(self.activations[-1], self.weights[-1]) + self.biases[-1]\n",
        "        self.z_values.append(z)\n",
        "        output = softmax(z)\n",
        "        self.activations.append(output)\n",
        "        return self.activations[-1]\n",
        "\n",
        "    def compute_loss(self, y_true, y_pred):\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        cross_entropy_loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
        "\n",
        "        # Add L2 regularization term\n",
        "        l2_reg_loss = 0\n",
        "        for w in self.weights:\n",
        "            l2_reg_loss += np.sum(np.square(w))\n",
        "        l2_reg_loss = (self.lambda_reg / (2 * y_true.shape[0])) * l2_reg_loss\n",
        "\n",
        "        total_loss = cross_entropy_loss + l2_reg_loss\n",
        "        return total_loss\n",
        "\n",
        "    def compute_accuracy(self, y_true, y_pred):\n",
        "        predictions = np.argmax(y_pred, axis=1)\n",
        "        true_labels = np.argmax(y_true, axis=1)\n",
        "        accuracy = np.mean(predictions == true_labels)\n",
        "        return accuracy\n",
        "\n",
        "    def backward(self, X, y, outputs):\n",
        "        m = X.shape[0]\n",
        "        self.d_weights = [np.zeros_like(w) for w in self.weights]\n",
        "        self.d_biases = [np.zeros_like(b) for b in self.biases]\n",
        "\n",
        "        dZ = outputs - y\n",
        "        # Add L2 regularization gradient\n",
        "        self.d_weights[-1] = (self.activations[-2].T @ dZ) / m + (self.lambda_reg / m) * self.weights[-1]\n",
        "        self.d_biases[-1] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        for i in range(len(self.weights) - 2, -1, -1):\n",
        "            dA_prev = dZ @ self.weights[i+1].T\n",
        "            dZ = dA_prev * relu_derivative(self.z_values[i])\n",
        "            # Add L2 regularization gradient\n",
        "            self.d_weights[i] = (self.activations[i].T @ dZ) / m + (self.lambda_reg / m) * self.weights[i]\n",
        "            self.d_biases[i] = np.sum(dZ, axis=0, keepdims=True) / m\n",
        "\n",
        "        # Adam optimization step\n",
        "        self.t += 1\n",
        "        for i in range(len(self.weights)):\n",
        "            # Update biased first and second moment estimates\n",
        "            self.m_weights[i] = self.beta1 * self.m_weights[i] + (1 - self.beta1) * self.d_weights[i]\n",
        "            self.v_weights[i] = self.beta2 * self.v_weights[i] + (1 - self.beta2) * np.square(self.d_weights[i])\n",
        "            self.m_biases[i] = self.beta1 * self.m_biases[i] + (1 - self.beta1) * self.d_biases[i]\n",
        "            self.v_biases[i] = self.beta2 * self.v_biases[i] + (1 - self.beta2) * np.square(self.d_biases[i])\n",
        "\n",
        "            # Compute bias-corrected first and second moment estimates\n",
        "            m_weights_corrected = self.m_weights[i] / (1 - self.beta1**self.t)\n",
        "            v_weights_corrected = self.v_weights[i] / (1 - self.beta2**self.t)\n",
        "            m_biases_corrected = self.m_biases[i] / (1 - self.beta1**self.t)\n",
        "            v_biases_corrected = self.v_biases[i] / (1 - self.beta2**self.t)\n",
        "\n",
        "            # Update weights and biases\n",
        "            self.weights[i] -= self.learning_rate * m_weights_corrected / (np.sqrt(v_weights_corrected) + self.epsilon)\n",
        "            self.biases[i] -= self.learning_rate * m_biases_corrected / (np.sqrt(v_biases_corrected) + self.epsilon)\n",
        "\n",
        "\n",
        "    def train(self, X, y, X_val, y_val, epochs, batch_size):\n",
        "        train_losses = []\n",
        "        val_losses = []\n",
        "        train_accuracies = []\n",
        "        val_accuracies = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            epoch_loss = 0\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                X_batch = X_shuffled[i:i+batch_size]\n",
        "                y_batch = y_shuffled[i:i+batch_size]\n",
        "\n",
        "                outputs = self.forward(X_batch)\n",
        "                epoch_loss += self.compute_loss(y_batch, outputs)\n",
        "                self.backward(X_batch, y_batch, outputs)\n",
        "\n",
        "            train_loss = epoch_loss / (X.shape[0] // batch_size)\n",
        "            train_pred = self.forward(X)\n",
        "            train_accuracy = self.compute_accuracy(y, train_pred)\n",
        "            val_pred = self.forward(X_val)\n",
        "            val_loss = self.compute_loss(y_val, val_pred)\n",
        "            val_accuracy = self.compute_accuracy(y_val, val_pred)\n",
        "\n",
        "            train_losses.append(train_loss)\n",
        "            val_losses.append(val_loss)\n",
        "            train_accuracies.append(train_accuracy)\n",
        "            val_accuracies.append(val_accuracy)\n",
        "\n",
        "            if epoch % 10 == 0:\n",
        "                print(f\"Epoch {epoch}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, \"\n",
        "                      f\"Train Acc: {train_accuracy:.4f}, Val Acc: {val_accuracy:.4f}\")\n",
        "\n",
        "        return train_losses, val_losses, train_accuracies, val_accuracies\n",
        "\n",
        "    def predict(self, X):\n",
        "        outputs = self.forward(X)\n",
        "        predictions = np.argmax(outputs, axis=1)\n",
        "        return predictions\n",
        "\n",
        "# Re-instantiate the model with Adam optimizer parameters and lambda_reg\n",
        "layer_sizes = [X_train.shape[1], 64, 32, num_classes]\n",
        "nn = MultiClassNeuralNetwork(layer_sizes, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, lambda_reg=0.001) # Adjusted learning rate for Adam and added Adam parameters\n",
        "\n",
        "train_losses, val_losses, train_accuracies, val_accuracies = nn.train(\n",
        "    X_train, y_train_one_hot, X_val, y_val_one_hot, epochs=100, batch_size=32\n",
        ")\n",
        "\n",
        "y_pred = nn.predict(X_test)\n",
        "print(\"\\nRapport de classification (Test set) :\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))\n",
        "\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "plt.title('Matrice de confusion (Test set)')\n",
        "plt.xlabel('Predit')\n",
        "plt.ylabel('Rel')\n",
        "plt.savefig('confusion_matrix.png')\n",
        "plt.close()\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "ax1.plot(train_losses, label='Train Loss')\n",
        "ax1.plot(val_losses, label='Validation Loss')\n",
        "ax1.set_title('Courbe de perte')\n",
        "ax1.set_xlabel('Epoque')\n",
        "ax1.set_ylabel('Perte')\n",
        "ax1.legend()\n",
        "\n",
        "ax2.plot(train_accuracies, label='Train Accuracy')\n",
        "ax2.plot(val_accuracies, label='Validation Accuracy')\n",
        "ax2.set_title('Courbe de précision')\n",
        "ax2.set_xlabel('Epoque')\n",
        "ax2.set_ylabel('Précision')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "fig.savefig('loss_accuracy_plot.png')\n",
        "plt.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1olmevkRIXU5",
        "outputId": "6ab3fed1-6c42-4778-f33c-43082055b40e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Train Loss: 1.8063, Val Loss: 1.0953, Train Acc: 0.6483, Val Acc: 0.6386\n",
            "Epoch 10, Train Loss: 0.5858, Val Loss: 0.6281, Train Acc: 0.8752, Val Acc: 0.8118\n",
            "Epoch 20, Train Loss: 0.5107, Val Loss: 0.5478, Train Acc: 0.9173, Val Acc: 0.8535\n",
            "Epoch 30, Train Loss: 0.4887, Val Loss: 0.6451, Train Acc: 0.9142, Val Acc: 0.8446\n",
            "Epoch 40, Train Loss: 0.4862, Val Loss: 0.6356, Train Acc: 0.9137, Val Acc: 0.8412\n",
            "Epoch 50, Train Loss: 0.4721, Val Loss: 0.6144, Train Acc: 0.9307, Val Acc: 0.8533\n",
            "Epoch 60, Train Loss: 0.4453, Val Loss: 0.7690, Train Acc: 0.9073, Val Acc: 0.8341\n",
            "Epoch 70, Train Loss: 0.5089, Val Loss: 0.5793, Train Acc: 0.9309, Val Acc: 0.8540\n",
            "Epoch 80, Train Loss: 0.4876, Val Loss: 0.6859, Train Acc: 0.9216, Val Acc: 0.8455\n",
            "Epoch 90, Train Loss: 0.4904, Val Loss: 0.6437, Train Acc: 0.9290, Val Acc: 0.8487\n",
            "\n",
            "Rapport de classification (Test set) :\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          ya       0.96      0.94      0.95       171\n",
            "         yab       0.83      0.71      0.77       171\n",
            "        yach       0.91      0.85      0.88       171\n",
            "         yad       0.85      0.87      0.86       171\n",
            "        yadd       0.74      0.70      0.72       171\n",
            "         yae       0.88      0.92      0.90       171\n",
            "         yaf       0.81      0.92      0.86       171\n",
            "         yag       0.82      0.91      0.86       171\n",
            "        yagg       0.97      0.93      0.95       170\n",
            "        yagh       0.92      0.98      0.95       170\n",
            "         yah       0.95      0.73      0.82       171\n",
            "        yahh       0.88      0.94      0.91       171\n",
            "         yaj       0.68      0.95      0.79       171\n",
            "         yak       0.84      0.87      0.85       171\n",
            "        yakk       0.93      0.96      0.95       171\n",
            "         yal       0.92      0.84      0.88       170\n",
            "         yam       0.95      0.75      0.84       171\n",
            "         yan       0.84      0.91      0.88       170\n",
            "         yaq       0.91      0.84      0.87       171\n",
            "         yar       0.74      0.87      0.80       171\n",
            "        yarr       0.82      0.87      0.85       171\n",
            "         yas       0.66      0.82      0.73       171\n",
            "        yass       0.95      0.73      0.83       171\n",
            "         yat       0.96      0.91      0.93       171\n",
            "        yatt       0.71      0.79      0.75       171\n",
            "         yaw       0.95      0.86      0.90       171\n",
            "         yax       0.95      0.74      0.83       170\n",
            "         yay       0.89      0.90      0.90       171\n",
            "         yaz       0.90      0.77      0.83       171\n",
            "        yazz       0.80      0.89      0.84       171\n",
            "         yey       0.83      0.81      0.82       171\n",
            "          yi       0.84      0.75      0.79       170\n",
            "          yu       0.82      0.89      0.85       171\n",
            "\n",
            "    accuracy                           0.85      5637\n",
            "   macro avg       0.86      0.85      0.85      5637\n",
            "weighted avg       0.86      0.85      0.85      5637\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
  
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Parameters for K-fold cross-validation\n",
        "n_splits = 5\n",
        "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "fold_results = []\n",
        "\n",
        "# Use the full dataset (X, y) for K-fold cross-validation\n",
        "# Ensure y is still the integer-encoded labels for splitting\n",
        "X_full = np.concatenate((X_train, X_val, X_test), axis=0)\n",
        "y_full = np.concatenate((y_train, y_val, y_test), axis=0)\n",
        "\n",
        "print(f\"Starting {n_splits}-fold cross-validation...\")\n",
        "\n",
        "fold_idx = 0\n",
        "for train_index, test_index in kf.split(X_full, y_full):\n",
        "    fold_idx += 1\n",
        "    print(f\"\\n--- Fold {fold_idx}/{n_splits} ---\")\n",
        "\n",
        "    X_train_fold, X_test_fold = X_full[train_index], X_full[test_index]\n",
        "    y_train_fold, y_test_fold = y_full[train_index], y_full[test_index]\n",
        "\n",
        "    # Convert labels to one-hot encoding for training and evaluation\n",
        "    y_train_fold_one_hot = one_hot_encoder.transform(y_train_fold.reshape(-1, 1))\n",
        "    y_test_fold_one_hot = one_hot_encoder.transform(y_test_fold.reshape(-1, 1))\n",
        "\n",
        "    # Re-initialize the neural network for each fold to ensure fresh start\n",
        "    nn_fold = MultiClassNeuralNetwork(layer_sizes, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8, lambda_reg=0.001)\n",
        "\n",
        "    # Split the training fold further into train and validation for this fold\n",
        "    # This helps monitor training progress during the fold's training,\n",
        "    # but the final evaluation for the fold is done on the fold's test set.\n",
        "    # A common practice is to use the test fold as validation during training\n",
        "    # or to split the training fold into train/validation internally.\n",
        "    # Here, for simplicity in demonstrating K-fold, we train on the entire train_fold\n",
        "    # and evaluate on the test_fold at the end of the fold's training.\n",
        "    # For more robust validation during training within each fold,\n",
        "    # another split (e.g., 80/20) could be applied to X_train_fold and y_train_fold_one_hot.\n",
        "    # We will train on the full train_fold and evaluate on the full test_fold.\n",
        "\n",
        "    print(f\"Training on {X_train_fold.shape[0]} samples, evaluating on {X_test_fold.shape[0]} samples for this fold.\")\n",
        "\n",
        "    # Train the model on the current fold's training data\n",
        "    # Pass X_test_fold and y_test_fold_one_hot as validation for monitoring purposes\n",
        "    train_losses_fold, val_losses_fold, train_accuracies_fold, val_accuracies_fold = nn_fold.train(\n",
        "        X_train_fold, y_train_fold_one_hot, X_test_fold, y_test_fold_one_hot, epochs=100, batch_size=32 # Using test fold as validation for monitoring\n",
        "    )\n",
        "\n",
        "    # Evaluate on the current fold's test data\n",
        "    y_pred_fold = nn_fold.predict(X_test_fold)\n",
        "\n",
        "    # Compute metrics for the current fold\n",
        "    fold_accuracy = nn_fold.compute_accuracy(y_test_fold_one_hot, nn_fold.forward(X_test_fold))\n",
        "    fold_loss = nn_fold.compute_loss(y_test_fold_one_hot, nn_fold.forward(X_test_fold))\n",
        "    fold_report = classification_report(y_test_fold, y_pred_fold, target_names=label_encoder.classes_, output_dict=True)\n",
        "\n",
        "    print(f\"Fold {fold_idx} Results:\")\n",
        "    print(f\"  Accuracy: {fold_accuracy:.4f}\")\n",
        "    print(f\"  Loss: {fold_loss:.4f}\")\n",
        "    # print(classification_report(y_test_fold, y_pred_fold, target_names=label_encoder.classes_)) # Detailed report per fold\n",
        "\n",
        "    fold_results.append({\n",
        "        'fold': fold_idx,\n",
        "        'accuracy': fold_accuracy,\n",
        "        'loss': fold_loss,\n",
        "        'report': fold_report,\n",
        "        'train_losses': train_losses_fold,\n",
        "        'val_losses': val_losses_fold,\n",
        "        'train_accuracies': train_accuracies_fold,\n",
        "        'val_accuracies': val_accuracies_fold\n",
        "    })\n",
        "\n",
        "# Aggregate and report results across all folds\n",
        "print(\"\\n--- K-fold Cross-Validation Summary ---\")\n",
        "\n",
        "all_fold_accuracies = [res['accuracy'] for res in fold_results]\n",
        "all_fold_losses = [res['loss'] for res in fold_results]\n",
        "\n",
        "print(f\"Average Accuracy across {n_splits} folds: {np.mean(all_fold_accuracies):.4f} (+/- {np.std(all_fold_accuracies):.4f})\")\n",
        "print(f\"Average Loss across {n_splits} folds: {np.mean(all_fold_losses):.4f} (+/- {np.std(all_fold_losses):.4f})\")\n",
        "\n",
        "# Optional: You can also aggregate classification reports or confusion matrices\n",
        "# This requires more complex aggregation logic if you need average precision/recall/f1 per class.\n",
        "# For simplicity, we just print average accuracy and loss.\n",
        "\n",
        "# Optional: Plot training/validation curves for each fold\n",
        "# for res in fold_results:\n",
        "#     plt.figure(figsize=(12, 5))\n",
        "#     plt.subplot(1, 2, 1)\n",
        "#     plt.plot(res['train_losses'], label='Train Loss')\n",
        "#     plt.plot(res['val_losses'], label='Fold Test Loss')\n",
        "#     plt.title(f'Fold {res[\"fold\"]} Loss Curve')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Loss')\n",
        "#     plt.legend()\n",
        "\n",
        "#     plt.subplot(1, 2, 2)\n",
        "#     plt.plot(res['train_accuracies'], label='Train Accuracy')\n",
        "#     plt.plot(res['val_accuracies'], label='Fold Test Accuracy')\n",
        "#     plt.title(f'Fold {res[\"fold\"]} Accuracy Curve')\n",
        "#     plt.xlabel('Epoch')\n",
        "#     plt.ylabel('Accuracy')\n",
        "#     plt.legend()\n",
        "#     plt.tight_layout()\n",
        "#     # plt.savefig(f'fold_{res[\"fold\"]}_loss_accuracy_plot.png')\n",
        "#     plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYvcP_E1UTbp",
        "outputId": "837a280d-35e9-448e-d009-f847f9413cfe"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting 5-fold cross-validation...\n",
            "\n",
            "Processing Fold 1/5\n",
            "Training model for this fold...\n",
            "Epoch 0, Train Loss: 2.4735, Val Loss: 1.9460, Train Acc: 0.4128, Val Acc: 0.3967\n",
            "Epoch 10, Train Loss: 0.5217, Val Loss: 0.5775, Train Acc: 0.8665, Val Acc: 0.8178\n",
            "Epoch 20, Train Loss: 0.2663, Val Loss: 0.4319, Train Acc: 0.9328, Val Acc: 0.8639\n",
            "Epoch 30, Train Loss: 0.1771, Val Loss: 0.3903, Train Acc: 0.9615, Val Acc: 0.8859\n",
            "Epoch 40, Train Loss: 0.1381, Val Loss: 0.4209, Train Acc: 0.9681, Val Acc: 0.8888\n",
            "Evaluating model on validation fold...\n",
            "Fold 1 Accuracy: 0.8996\n",
            "\n",
            "Processing Fold 2/5\n",
            "Training model for this fold...\n",
            "Epoch 0, Train Loss: 2.5062, Val Loss: 1.9127, Train Acc: 0.3969, Val Acc: 0.4018\n",
            "Epoch 10, Train Loss: 0.5755, Val Loss: 0.6394, Train Acc: 0.8399, Val Acc: 0.7903\n",
            "Epoch 20, Train Loss: 0.3193, Val Loss: 0.4484, Train Acc: 0.9206, Val Acc: 0.8590\n",
            "Epoch 30, Train Loss: 0.2194, Val Loss: 0.3861, Train Acc: 0.9474, Val Acc: 0.8819\n",
            "Epoch 40, Train Loss: 0.1650, Val Loss: 0.3838, Train Acc: 0.9593, Val Acc: 0.8886\n",
            "Evaluating model on validation fold...\n",
            "Fold 2 Accuracy: 0.8911\n",
            "\n",
            "Processing Fold 3/5\n",
            "Training model for this fold...\n",
            "Epoch 0, Train Loss: 2.6457, Val Loss: 2.0708, Train Acc: 0.3558, Val Acc: 0.3597\n",
            "Epoch 10, Train Loss: 0.6042, Val Loss: 0.5935, Train Acc: 0.8431, Val Acc: 0.8149\n",
            "Epoch 20, Train Loss: 0.3185, Val Loss: 0.4015, Train Acc: 0.9220, Val Acc: 0.8685\n",
            "Epoch 30, Train Loss: 0.2110, Val Loss: 0.3606, Train Acc: 0.9556, Val Acc: 0.8925\n",
            "Epoch 40, Train Loss: 0.1549, Val Loss: 0.3803, Train Acc: 0.9694, Val Acc: 0.8909\n",
            "Evaluating model on validation fold...\n",
            "Fold 3 Accuracy: 0.8957\n",
            "\n",
            "Processing Fold 4/5\n",
            "Training model for this fold...\n",
            "Epoch 0, Train Loss: 2.5468, Val Loss: 2.0476, Train Acc: 0.3739, Val Acc: 0.3707\n",
            "Epoch 10, Train Loss: 0.5776, Val Loss: 0.6295, Train Acc: 0.8418, Val Acc: 0.8025\n",
            "Epoch 20, Train Loss: 0.3127, Val Loss: 0.4647, Train Acc: 0.9130, Val Acc: 0.8591\n",
            "Epoch 30, Train Loss: 0.2083, Val Loss: 0.3966, Train Acc: 0.9489, Val Acc: 0.8824\n",
            "Epoch 40, Train Loss: 0.1557, Val Loss: 0.4407, Train Acc: 0.9608, Val Acc: 0.8845\n",
            "Evaluating model on validation fold...\n",
            "Fold 4 Accuracy: 0.8934\n",
            "\n",
            "Processing Fold 5/5\n",
            "Training model for this fold...\n",
            "Epoch 0, Train Loss: 2.6041, Val Loss: 2.0185, Train Acc: 0.3795, Val Acc: 0.3623\n",
            "Epoch 10, Train Loss: 0.5905, Val Loss: 0.6465, Train Acc: 0.8317, Val Acc: 0.7874\n",
            "Epoch 20, Train Loss: 0.3058, Val Loss: 0.4461, Train Acc: 0.9206, Val Acc: 0.8598\n",
            "Epoch 30, Train Loss: 0.1975, Val Loss: 0.4050, Train Acc: 0.9517, Val Acc: 0.8769\n",
            "Epoch 40, Train Loss: 0.1510, Val Loss: 0.4064, Train Acc: 0.9664, Val Acc: 0.8854\n",
            "Evaluating model on validation fold...\n",
            "Fold 5 Accuracy: 0.8868\n",
            "\n",
            "Cross-validation finished.\n",
            "Average accuracy across 5 folds: 0.8933\n",
            "Standard deviation of accuracy: 0.0043\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
       
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "def rotate_image(image, angle):\n",
        "    \"\"\"Applique une rotation à une image.\"\"\"\n",
        "    # Ensure image is 2D (grayscale)\n",
        "    if len(image.shape) > 2:\n",
        "        raise ValueError(\"Image must be grayscale for this rotation function\")\n",
        "\n",
        "    rows, cols = image.shape\n",
        "    M = cv2.getRotationMatrix2D(((cols-1)/2.0, (rows-1)/2.0), angle, 1)\n",
        "    # Use BORDER_REPLICATE to avoid black borders\n",
        "    rotated_image = cv2.warpAffine(image, M, (cols, rows), borderMode=cv2.BORDER_REPLICATE)\n",
        "    return rotated_image\n",
        "\n",
        "def translate_image(image, tx, ty):\n",
        "    \"\"\"Applique une translation à une image.\"\"\"\n",
        "    # Ensure image is 2D (grayscale)\n",
        "    if len(image.shape) > 2:\n",
        "        raise ValueError(\"Image must be grayscale for this translation function\")\n",
        "\n",
        "    rows, cols = image.shape\n",
        "    M = np.float32([[1, 0, tx], [0, 1, ty]])\n",
        "    # Use BORDER_REPLICATE to avoid black borders\n",
        "    translated_image = cv2.warpAffine(image, M, (cols, rows), borderMode=cv2.BORDER_REPLICATE)\n",
        "    return translated_image\n",
        "\n",
        "def augment_data(X, y, rotations=[-10, 10], translations=[(-5, 0), (5, 0), (0, -5), (0, 5)]):\n",
        "    \"\"\"Applique des augmentations (rotations et translations) aux données.\"\"\"\n",
        "    X_augmented = []\n",
        "    y_augmented = []\n",
        "\n",
        "    # Original data\n",
        "    X_augmented.extend(X)\n",
        "    y_augmented.extend(y)\n",
        "\n",
        "    # Augment with rotations\n",
        "    for angle in rotations:\n",
        "        for img, label in zip(X, y):\n",
        "            # Reshape the flattened image back to its original 2D shape\n",
        "            original_shape = (int(np.sqrt(img.shape[0])), int(np.sqrt(img.shape[0]))) # Assuming square images\n",
        "            img_2d = img.reshape(original_shape)\n",
        "\n",
        "            rotated_img = rotate_image(img_2d, angle)\n",
        "\n",
        "            # Flatten the augmented image and append\n",
        "            X_augmented.append(rotated_img.flatten())\n",
        "            y_augmented.append(label)\n",
        "\n",
        "    # Augment with translations\n",
        "    for tx, ty in translations:\n",
        "        for img, label in zip(X, y):\n",
        "            # Reshape the flattened image back to its original 2D shape\n",
        "            original_shape = (int(np.sqrt(img.shape[0])), int(np.sqrt(img.shape[0]))) # Assuming square images\n",
        "            img_2d = img.reshape(original_shape)\n",
        "\n",
        "            translated_img = translate_image(img_2d, tx, ty)\n",
        "\n",
        "            # Flatten the augmented image and append\n",
        "            X_augmented.append(translated_img.flatten())\n",
        "            y_augmented.append(label)\n",
        "\n",
        "\n",
        "    return np.array(X_augmented), np.array(y_augmented)\n",
        "\n",
        "# Example Usage after splitting data:\n",
        "# Assuming X_train and y_train are already loaded and preprocessed as flat arrays\n",
        "\n",
        "print(f\"Original training data size: {X_train.shape[0]}\")\n",
        "\n",
        "# Apply augmentation to the training data\n",
        "X_train_aug, y_train_aug = augment_data(X_train, y_train,\n",
        "                                        rotations=[-15, 15], # Example rotation angles\n",
        "                                        translations=[(-5, 0), (5, 0), (0, -5), (0, 5)]) # Example translation shifts\n",
        "\n",
        "print(f\"Augmented training data size: {X_train_aug.shape[0]}\")\n",
        "\n",
        "# Convert the augmented labels to one-hot encoding\n",
        "y_train_aug_one_hot = one_hot_encoder.transform(y_train_aug.reshape(-1, 1))\n",
        "\n",
        "# Now you can train your model using the augmented data:\n",
        "# layer_sizes = [X_train_aug.shape[1], 64, 32, num_classes] # Make sure layer size matches augmented data if necessary (should be same here)\n",
        "# nn_aug = MultiClassNeuralNetwork(layer_sizes, learning_rate=... , beta1=..., beta2=..., epsilon=..., lambda_reg=...)\n",
        "# train_losses_aug, val_losses_aug, train_accuracies_aug, val_accuracies_aug = nn_aug.train(\n",
        "#     X_train_aug, y_train_aug_one_hot, X_val, y_val_one_hot, epochs=..., batch_size=...\n",
        "# )\n",
        "\n",
        "# Note: Augmentation is typically applied only to the training data.\n",
        "# Validation and Test sets remain unchanged to provide an unbiased evaluation.\n",
        "\n",
        "# Optional: Visualize some augmented images to verify the process\n",
        "# num_to_visualize = 5\n",
        "# for i in range(num_to_visualize):\n",
        "#     plt.figure(figsize=(1, 1))\n",
        "#     # Reshape the flattened image back to its original 2D shape\n",
        "#     img_aug_2d = X_train_aug[i].reshape((int(np.sqrt(X_train_aug[i].shape[0])), int(np.sqrt(X_train_aug[i].shape[0]))))\n",
        "#     plt.imshow(img_aug_2d, cmap='gray')\n",
        "#     plt.title(f\"Augmented: {label_encoder.inverse_transform([y_train_aug[i]])[0]}\")\n",
        "#     plt.axis('off')\n",
        "#     plt.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJBbY6ozlENj",
        "outputId": "79ece5d4-e643-4a99-8181-a4b970901c81"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original training data size: 16908\n",
            "Augmented training data size: 118356\n"
          ]
        }
      ]
    }
  ]
}
